#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Academic Compliance Checker for √úbung 5 ML Implementation

This script validates all implemented ML tasks against the official requirements
for Sections 6.1, 6.2, and 6.3 to ensure academic compliance.
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import json
import logging

# Colors for beautiful output
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
BLUE = '\033[0;34m'
CYAN = '\033[0;36m'
RED = '\033[0;31m'
WHITE = '\033[1;37m'
PURPLE = '\033[0;35m'
NC = '\033[0m'

def get_log_path(filename):
    """Get path for log files"""
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    logs_dir = os.path.join(project_root, 'logs')
    os.makedirs(logs_dir, exist_ok=True)
    return os.path.join(logs_dir, filename)

def get_report_path(filename):
    """Get path for report files"""
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    reports_dir = os.path.join(project_root, 'docs', 'reports')
    os.makedirs(reports_dir, exist_ok=True)
    return os.path.join(reports_dir, filename)

def print_header(message):
    print(f"{PURPLE}{'‚ïî' + '‚ïê' * 68 + '‚ïó'}{NC}")
    print(f"{PURPLE}‚ïë{WHITE}{message:^68}{PURPLE}‚ïë{NC}")
    print(f"{PURPLE}{'‚ïö' + '‚ïê' * 68 + '‚ïù'}{NC}")

def print_section(message):
    print(f"{CYAN}üìã {message}{NC}")

def print_success(message):
    print(f"{GREEN}‚úÖ {message}{NC}")

def print_info(message):
    print(f"{YELLOW}üìä {message}{NC}")

def print_error(message):
    print(f"{RED}‚ùå {message}{NC}")

# Setup project root
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class UebungTaskChecker:
    """Comprehensive checker for √úbung 5 ML implementation tasks"""
    
    def __init__(self):
        self.setup_logging()
        self.results = {
            '6.1': {'tasks': {}, 'score': 0, 'max_score': 0},
            '6.2': {'tasks': {}, 'score': 0, 'max_score': 0},
            '6.3': {'tasks': {}, 'score': 0, 'max_score': 0}
        }
        self.ml_dir = os.path.join(project_root, 'src', 'ml')
        self.data_dir = os.path.join(project_root, 'data')
        self.logs_dir = os.path.join(project_root, 'logs')
        self.reports_dir = os.path.join(project_root, 'docs', 'reports')
        self.viz_dir = os.path.join(project_root, 'docs', 'visualizations')
        
    def setup_logging(self):
        """Setup logging configuration"""
        log_path = get_log_path('ml_task_checker.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('TaskChecker')
        
    def check_file_exists(self, filepath, description=""):
        """Helper to check if file exists"""
        exists = os.path.exists(filepath)
        if exists:
            self.logger.info(f"‚úÖ Found: {description} ({filepath})")
        else:
            self.logger.warning(f"‚ùå Missing: {description} ({filepath})")
        return exists
        
    def check_task_6_1_data_preparation(self):
        """
        Section 6.1: Vorbereitung der Daten f√ºr maschinelles Lernen
        Check data preparation and ML readiness tasks
        """
        self.logger.info("üîç Checking Section 6.1: Data Preparation for ML")
        section = '6.1'
        
        # Task 6.1.1: Define clinically relevant prediction task
        task_name = "Clinical Prediction Task Definition"
        self.logger.info(f"üìã Checking: {task_name}")
        
        # Check if mortality extraction exists and defines prediction task
        mortality_script = os.path.join(self.ml_dir, 'step2_mortality_extraction.py')
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        task_defined = False
        if self.check_file_exists(mortality_script, "Mortality extraction script"):
            # Read the script to check for prediction task definition
            with open(mortality_script, 'r') as f:
                content = f.read()
                if "48-hour mortality" in content and "prediction" in content:
                    task_defined = True
                    
        if self.check_file_exists(readme_path, "ML README documentation"):
            with open(readme_path, 'r') as f:
                readme_content = f.read()
                if "48-hour ICU mortality prediction" in readme_content:
                    task_defined = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': task_defined,
            'details': "48-hour ICU mortality prediction task clearly defined" if task_defined else "Prediction task not clearly defined",
            'points': 2 if task_defined else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if task_defined:
            self.results[section]['score'] += 2
            
        # Task 6.1.2: Analyze class distribution
        task_name = "Class Distribution Analysis"
        self.logger.info(f"üìã Checking: {task_name}")
        
        eda_script = os.path.join(self.ml_dir, 'step3_exploratory_analysis.py')
        eda_report = os.path.join(self.reports_dir, 'ml_eda_report.md')
        class_analysis_done = False
        
        if self.check_file_exists(eda_script, "EDA script"):
            with open(eda_script, 'r') as f:
                content = f.read()
                if "class_imbalance_analysis" in content and "imbalance_ratio" in content:
                    class_analysis_done = True
                    
        if self.check_file_exists(eda_report, "EDA report"):
            with open(eda_report, 'r') as f:
                content = f.read()
                if "Class Imbalance" in content and "13.2:1" in content:
                    class_analysis_done = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': class_analysis_done,
            'details': "Class imbalance (13.2:1) analyzed and documented" if class_analysis_done else "Class distribution analysis missing",
            'points': 2 if class_analysis_done else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if class_analysis_done:
            self.results[section]['score'] += 2
            
        # Task 6.1.3: Temporal data splitting
        task_name = "Temporal Data Splitting"
        self.logger.info(f"üìã Checking: {task_name}")
        
        feature_script = os.path.join(self.ml_dir, 'step4_feature_engineering.py')
        processed_data = os.path.join(self.data_dir, 'processed')
        temporal_split_done = False
        
        if self.check_file_exists(feature_script, "Feature engineering script"):
            with open(feature_script, 'r') as f:
                content = f.read()
                if "temporal_train_test_split" in content and "prediction_timepoint" in content:
                    temporal_split_done = True
                    
        # Check if processed split files exist
        split_files = ['X_train.csv', 'X_val.csv', 'X_test.csv', 'y_train.csv', 'y_val.csv', 'y_test.csv']
        if os.path.exists(processed_data):
            all_splits_exist = all(os.path.exists(os.path.join(processed_data, f)) for f in split_files)
            if all_splits_exist:
                temporal_split_done = True
                
        self.results[section]['tasks'][task_name] = {
            'completed': temporal_split_done,
            'details': "Temporal train/validation/test split implemented" if temporal_split_done else "Temporal splitting not implemented",
            'points': 3 if temporal_split_done else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if temporal_split_done:
            self.results[section]['score'] += 3
            
        # Task 6.1.4: Handle class imbalance strategy
        task_name = "Class Imbalance Handling Strategy"
        self.logger.info(f"üìã Checking: {task_name}")
        
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        imbalance_handled = False
        
        if self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                if "SMOTE" in content or "class_weight" in content:
                    imbalance_handled = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': imbalance_handled,
            'details': "SMOTE or class weighting implemented" if imbalance_handled else "Class imbalance handling missing",
            'points': 2 if imbalance_handled else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if imbalance_handled:
            self.results[section]['score'] += 2
            
    def check_task_6_2_model_implementation(self):
        """
        Section 6.2: Implementierung und Training von ML-Modellen
        Check model implementation and training tasks
        """
        self.logger.info("üîç Checking Section 6.2: Model Implementation & Training")
        section = '6.2'
        
        # Task 6.2.1: Baseline Model Implementation
        task_name = "Baseline Model Implementation"
        self.logger.info(f"üìã Checking: {task_name}")
        
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        baseline_implemented = False
        baseline_models = []
        
        if self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                models = ['LogisticRegression', 'RandomForest', 'GradientBoosting', 'SVM']
                found_models = [model for model in models if model in content]
                if len(found_models) >= 2:  # At least 2 baseline models
                    baseline_implemented = True
                    baseline_models = found_models
                    
        self.results[section]['tasks'][task_name] = {
            'completed': baseline_implemented,
            'details': f"Baseline models implemented: {', '.join(baseline_models)}" if baseline_implemented else "Baseline models missing",
            'points': 3 if baseline_implemented else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if baseline_implemented:
            self.results[section]['score'] += 3
            
        # Task 6.2.2: Advanced ML Model Implementation
        task_name = "Advanced ML Model Implementation"
        self.logger.info(f"üìã Checking: {task_name}")
        
        # Check for advanced models (Random Forest, Gradient Boosting are considered advanced)
        advanced_implemented = False
        if baseline_implemented and ('RandomForest' in baseline_models or 'GradientBoosting' in baseline_models):
            advanced_implemented = True
            
        self.results[section]['tasks'][task_name] = {
            'completed': advanced_implemented,
            'details': "Random Forest and Gradient Boosting implemented" if advanced_implemented else "Advanced models missing",
            'points': 3 if advanced_implemented else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if advanced_implemented:
            self.results[section]['score'] += 3
            
        # Task 6.2.3: Model Training with Monitoring
        task_name = "Model Training & Monitoring"
        self.logger.info(f"üìã Checking: {task_name}")
        
        training_implemented = False
        models_saved = False
        
        # Check for training implementation
        if self.check_file_exists(baseline_script, "Training script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                if "cross_validation" in content or "GridSearchCV" in content:
                    training_implemented = True
                if "pickle.dump" in content or "joblib.dump" in content:
                    models_saved = True
                    
        # Check if models directory exists with saved models
        models_dir = os.path.join(project_root, 'models')
        if os.path.exists(models_dir) and os.listdir(models_dir):
            models_saved = True
            
        training_complete = training_implemented and models_saved
        
        self.results[section]['tasks'][task_name] = {
            'completed': training_complete,
            'details': f"Training: {'‚úÖ' if training_implemented else '‚ùå'}, Model Saving: {'‚úÖ' if models_saved else '‚ùå'}",
            'points': 2 if training_complete else 1 if training_implemented else 0,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        if training_complete:
            self.results[section]['score'] += 2
        elif training_implemented:
            self.results[section]['score'] += 1
            
        # Task 6.2.4: Model Justification
        task_name = "Model Selection Justification"
        self.logger.info(f"üìã Checking: {task_name}")
        
        justification_found = False
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        if self.check_file_exists(readme_path, "ML README"):
            with open(readme_path, 'r') as f:
                content = f.read()
                if "Random Forest" in content and "clinical" in content.lower():
                    justification_found = True
                    
        self.results[section]['tasks'][task_name] = {
            'completed': justification_found,
            'details': "Model selection justified in documentation" if justification_found else "Model justification missing",
            'points': 1 if justification_found else 0,
            'max_points': 1
        }
        self.results[section]['max_score'] += 1
        if justification_found:
            self.results[section]['score'] += 1
            
    def check_task_6_3_evaluation_and_xai(self):
        """
        Section 6.3: Modellbewertung und Explainable AI
        Check model evaluation and XAI implementation
        """
        self.logger.info("üîç Checking Section 6.3: Model Evaluation & Explainable AI")
        section = '6.3'
        
        # Task 6.3.1: Rigorous Model Evaluation
        task_name = "Model Performance Evaluation"
        self.logger.info(f"üìã Checking: {task_name}")
        
        enhanced_models_script = os.path.join(self.ml_dir, 'step5_enhanced_models.py')
        baseline_script = os.path.join(self.ml_dir, 'step5_baseline_models.py')
        ml_report = os.path.join(self.reports_dir, 'ml_enhanced_models_report.md')
        
        evaluation_done = False
        metrics_used = []
        
        # Check enhanced models script first
        if self.check_file_exists(enhanced_models_script, "Enhanced models script"):
            with open(enhanced_models_script, 'r') as f:
                content = f.read()
                required_metrics = ['roc_auc_score', 'f1_score', 'precision_score', 'recall_score', 'average_precision_score']
                found_metrics = [metric for metric in required_metrics if metric in content]
                if len(found_metrics) >= 4:  # At least 4 metrics
                    evaluation_done = True
                    metrics_used = found_metrics
        
        # Fallback to baseline script
        elif self.check_file_exists(baseline_script, "Baseline models script"):
            with open(baseline_script, 'r') as f:
                content = f.read()
                required_metrics = ['roc_auc_score', 'f1_score', 'precision_score', 'recall_score']
                found_metrics = [metric for metric in required_metrics if metric in content]
                if len(found_metrics) >= 3:  # At least 3 metrics
                    evaluation_done = True
                    metrics_used = found_metrics
                    
        self.results[section]['tasks'][task_name] = {
            'completed': evaluation_done,
            'details': f"Comprehensive metrics implemented: {', '.join(metrics_used)}" if evaluation_done else "Comprehensive evaluation missing",
            'points': 3 if evaluation_done else 0,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        if evaluation_done:
            self.results[section]['score'] += 3
            
        # Task 6.3.2: Explainable AI Implementation
        task_name = "Explainable AI (XAI) Implementation"
        self.logger.info(f"üìã Checking: {task_name}")
        
        # Check for dedicated XAI implementation
        xai_script = os.path.join(self.ml_dir, 'step6_explainable_ai.py')
        xai_report = os.path.join(self.reports_dir, 'ml_xai_report.md')
        xai_viz_dir = os.path.join(self.viz_dir, 'xai')
        
        xai_implemented = False
        xai_methods = []
        xai_score = 0
        
        # Check for dedicated XAI script
        if self.check_file_exists(xai_script, "Dedicated XAI script"):
            with open(xai_script, 'r') as f:
                content = f.read()
                if 'SHAP' in content or 'shap' in content:
                    xai_implemented = True
                    xai_methods.append('SHAP')
                    xai_score += 2
                if 'TreeExplainer' in content and 'LinearExplainer' in content:
                    xai_methods.append('Multiple SHAP Explainers')
                    xai_score += 1
                if 'patient_level' in content.lower() or 'patient' in content.lower():
                    xai_methods.append('Patient-level Explanations')
                    xai_score += 1
        
        # Check for XAI report and visualizations
        if self.check_file_exists(xai_report, "XAI report"):
            xai_score += 1
            
        if os.path.exists(xai_viz_dir) and len(os.listdir(xai_viz_dir)) > 0:
            xai_viz_count = len([f for f in os.listdir(xai_viz_dir) if f.endswith('.png')])
            if xai_viz_count >= 5:
                xai_methods.append(f'{xai_viz_count} XAI visualizations')
                xai_score += 1
                
        # Fallback: check baseline models for basic feature importance
        if not xai_implemented:
            if self.check_file_exists(baseline_script, "Baseline script with feature importance"):
                with open(baseline_script, 'r') as f:
                    content = f.read()
                    if 'feature_importances_' in content:
                        xai_methods.append('Feature Importance')
                        xai_score = 1
                        
        self.results[section]['tasks'][task_name] = {
            'completed': xai_implemented,
            'details': f"XAI methods implemented: {', '.join(xai_methods)}" if xai_methods else "XAI implementation missing",
            'points': min(xai_score, 4),
            'max_points': 4
        }
        self.results[section]['max_score'] += 4
        self.results[section]['score'] += min(xai_score, 4)
            
        # Task 6.3.3: Clinical Patient Examples Analysis
        task_name = "Clinical Patient Examples Analysis"
        self.logger.info(f"üìã Checking: {task_name}")
        
        examples_analyzed = False
        examples_score = 0
        
        # Check for patient-level analysis in XAI
        if os.path.exists(xai_script):
            with open(xai_script, 'r') as f:
                content = f.read()
                if 'patient_level' in content.lower() and 'clinical' in content.lower():
                    examples_analyzed = True
                    examples_score += 2
                if 'true_positive' in content.lower() and 'false_positive' in content.lower():
                    examples_score += 1
        
        # Check for patient visualizations
        if os.path.exists(xai_viz_dir):
            patient_viz = [f for f in os.listdir(xai_viz_dir) if 'patient' in f.lower() or 'explanation' in f.lower()]
            if len(patient_viz) >= 3:
                examples_analyzed = True
                examples_score = max(examples_score, 2)
                
        # Fallback: check general visualizations
        viz_files = []
        if os.path.exists(self.viz_dir):
            for root, dirs, files in os.walk(self.viz_dir):
                viz_files.extend([os.path.join(root, f) for f in files if f.endswith('.png')])
                
        if len(viz_files) > 10:  # Many visualizations suggest comprehensive analysis
            examples_score = max(examples_score, 1)
            
        self.results[section]['tasks'][task_name] = {
            'completed': examples_analyzed,
            'details': f"Patient analysis implemented with {len(viz_files)} total visualizations" if examples_score > 0 else "Patient examples analysis missing",
            'points': examples_score,
            'max_points': 3
        }
        self.results[section]['max_score'] += 3
        self.results[section]['score'] += examples_score
            
        # Task 6.3.4: Clinical Usefulness Assessment
        task_name = "Clinical Usefulness Assessment"
        self.logger.info(f"üìã Checking: {task_name}")
        
        clinical_assessment = False
        assessment_score = 0
        
        # Check XAI report for clinical discussion
        if self.check_file_exists(xai_report, "XAI report"):
            with open(xai_report, 'r') as f:
                content = f.read()
                if "clinical" in content.lower() and ("decision" in content.lower() or "usefulness" in content.lower()):
                    clinical_assessment = True
                    assessment_score += 1
                if "recommendations" in content.lower():
                    assessment_score += 1
                    
        # Check main README for clinical context
        readme_path = os.path.join(self.ml_dir, 'README.md')
        if self.check_file_exists(readme_path, "ML README"):
            with open(readme_path, 'r') as f:
                content = f.read()
                if "clinical" in content.lower() and ("decision" in content.lower() or "usefulness" in content.lower()):
                    clinical_assessment = True
                    assessment_score = max(assessment_score, 1)
                    
        self.results[section]['tasks'][task_name] = {
            'completed': clinical_assessment,
            'details': "Clinical usefulness discussed in documentation" if clinical_assessment else "Clinical assessment missing",
            'points': assessment_score,
            'max_points': 2
        }
        self.results[section]['max_score'] += 2
        self.results[section]['score'] += assessment_score
            
    def check_implementation_quality(self):
        """Check overall implementation quality and academic standards"""
        self.logger.info("üîç Checking Implementation Quality & Academic Standards")
        
        quality_checks = {
            'Code Organization': self.check_code_organization(),
            'Documentation Quality': self.check_documentation_quality(),
            'Logging & Reporting': self.check_logging_reporting(),
            'Reproducibility': self.check_reproducibility(),
            'Academic Standards': self.check_academic_standards()
        }
        
        return quality_checks
        
    def check_code_organization(self):
        """Check code organization and structure"""
        expected_files = [
            'step1_environment_setup.py',
            'step2_mortality_extraction.py',
            'step3_exploratory_analysis.py',
            'step4_feature_engineering.py',
            'step5_baseline_models.py',
            'README.md',
            'requirements_ml.txt'
        ]
        
        existing_files = [f for f in expected_files if os.path.exists(os.path.join(self.ml_dir, f))]
        organization_score = len(existing_files) / len(expected_files)
        
        return {
            'score': organization_score,
            'details': f"{len(existing_files)}/{len(expected_files)} expected files present",
            'files': existing_files
        }
        
    def check_documentation_quality(self):
        """Check documentation completeness"""
        readme_path = os.path.join(self.ml_dir, 'README.md')
        
        if not os.path.exists(readme_path):
            return {'score': 0, 'details': "README.md missing"}
            
        with open(readme_path, 'r') as f:
            content = f.read()
            
        quality_indicators = [
            'Objective' in content,
            'Requirements' in content,
            'Pipeline Architecture' in content,
            'Quick Start' in content,
            'Results' in content,
            'Next Steps' in content
        ]
        
        score = sum(quality_indicators) / len(quality_indicators)
        
        return {
            'score': score,
            'details': f"{sum(quality_indicators)}/{len(quality_indicators)} documentation sections present"
        }
        
    def check_logging_reporting(self):
        """Check logging and reporting implementation"""
        log_files = []
        report_files = []
        
        if os.path.exists(self.logs_dir):
            log_files = [f for f in os.listdir(self.logs_dir) if f.startswith('ml_') and f.endswith('.log')]
            
        if os.path.exists(self.reports_dir):
            report_files = [f for f in os.listdir(self.reports_dir) if f.startswith('ml_') and f.endswith('.md')]
            
        total_expected = 5  # 5 ML steps should generate logs/reports
        actual_logs = len(log_files)
        actual_reports = len(report_files)
        
        score = (actual_logs + actual_reports) / (2 * total_expected)
        
        return {
            'score': min(score, 1.0),
            'details': f"Logs: {actual_logs}, Reports: {actual_reports}",
            'log_files': log_files,
            'report_files': report_files
        }
        
    def check_reproducibility(self):
        """Check reproducibility features"""
        reproducibility_features = []
        
        # Check for requirements file
        if os.path.exists(os.path.join(self.ml_dir, 'requirements_ml.txt')):
            reproducibility_features.append('Requirements file')
            
        # Check for saved processed data
        if os.path.exists(os.path.join(self.data_dir, 'processed')):
            reproducibility_features.append('Processed data saved')
            
        # Check for saved models
        if os.path.exists(os.path.join(project_root, 'models')):
            reproducibility_features.append('Models saved')
            
        # Check for metadata/scalers
        processed_dir = os.path.join(self.data_dir, 'processed')
        if os.path.exists(os.path.join(processed_dir, 'metadata.pickle')):
            reproducibility_features.append('Metadata preserved')
            
        if os.path.exists(os.path.join(processed_dir, 'scalers.pickle')):
            reproducibility_features.append('Scalers saved')
            
        score = len(reproducibility_features) / 5
        
        return {
            'score': score,
            'details': f"{len(reproducibility_features)}/5 reproducibility features",
            'features': reproducibility_features
        }
        
    def check_academic_standards(self):
        """Check adherence to academic standards"""
        standards_met = []
        
        # Check for proper citations/references in README
        readme_path = os.path.join(self.ml_dir, 'README.md')
        if os.path.exists(readme_path):
            with open(readme_path, 'r') as f:
                content = f.read()
                if 'References' in content:
                    standards_met.append('References provided')
                if 'limitations' in content.lower() or 'known issues' in content.lower():
                    standards_met.append('Limitations discussed')
                if 'professor approved' in content.lower():
                    standards_met.append('Library restrictions followed')
                    
        # Check for step-by-step implementation
        ml_files = [f for f in os.listdir(self.ml_dir) if f.startswith('step') and f.endswith('.py')]
        if len(ml_files) >= 5:
            standards_met.append('Step-by-step implementation')
            
        score = len(standards_met) / 4
        
        return {
            'score': score,
            'details': f"{len(standards_met)}/4 academic standards met",
            'standards': standards_met
        }
        
    def generate_comprehensive_report(self):
        """Generate comprehensive task completion report"""
        total_score = sum(section['score'] for section in self.results.values())
        total_max_score = sum(section['max_score'] for section in self.results.values())
        overall_percentage = (total_score / total_max_score * 100) if total_max_score > 0 else 0
        
        # Check implementation quality
        quality_checks = self.check_implementation_quality()
        
        report_path = get_report_path('ml_task_checker_report.md')
        
        with open(report_path, 'w') as f:
            f.write("# üìã √úbung 5 - ML Task Completion Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## üéØ Overall Completion Status\n\n")
            f.write(f"**Total Score:** {total_score}/{total_max_score} ({overall_percentage:.1f}%)\n\n")
            
            # Determine overall status
            if overall_percentage >= 90:
                status = "üü¢ EXCELLENT"
            elif overall_percentage >= 80:
                status = "üü° GOOD"  
            elif overall_percentage >= 70:
                status = "üü† SATISFACTORY"
            elif overall_percentage >= 60:
                status = "üî¥ NEEDS IMPROVEMENT"
            else:
                status = "‚ùå INCOMPLETE"
                
            f.write(f"**Status:** {status}\n\n")
            
            # Section-by-section breakdown
            for section_num, section_data in self.results.items():
                section_percentage = (section_data['score'] / section_data['max_score'] * 100) if section_data['max_score'] > 0 else 0
                f.write(f"## üìä Section {section_num} Results ({section_percentage:.1f}%)\n\n")
                
                if section_num == '6.1':
                    f.write("**Focus:** Data Preparation for ML\n\n")
                elif section_num == '6.2':
                    f.write("**Focus:** Model Implementation & Training\n\n")
                elif section_num == '6.3':
                    f.write("**Focus:** Model Evaluation & Explainable AI\n\n")
                    
                f.write(f"**Score:** {section_data['score']}/{section_data['max_score']}\n\n")
                
                for task_name, task_data in section_data['tasks'].items():
                    status_icon = "‚úÖ" if task_data['completed'] else "‚ùå"
                    f.write(f"### {status_icon} {task_name}\n")
                    f.write(f"**Points:** {task_data['points']}/{task_data['max_points']}\n")
                    f.write(f"**Details:** {task_data['details']}\n\n")
                    
            # Implementation Quality Assessment
            f.write("## üîß Implementation Quality Assessment\n\n")
            
            for check_name, check_data in quality_checks.items():
                score_percentage = check_data['score'] * 100
                status_icon = "‚úÖ" if score_percentage >= 80 else "üü°" if score_percentage >= 60 else "‚ùå"
                f.write(f"### {status_icon} {check_name} ({score_percentage:.1f}%)\n")
                f.write(f"**Details:** {check_data['details']}\n\n")
                
            # Recommendations
            f.write("## üí° Recommendations\n\n")
            
            if overall_percentage < 100:
                f.write("### Next Steps to Complete Implementation:\n")
                
                # Check what's missing
                missing_tasks = []
                for section_num, section_data in self.results.items():
                    for task_name, task_data in section_data['tasks'].items():
                        if not task_data['completed']:
                            missing_tasks.append(f"**{section_num}:** {task_name}")
                            
                if missing_tasks:
                    for task in missing_tasks:
                        f.write(f"- {task}\n")
                        
                f.write("\n### Priority Actions:\n")
                if overall_percentage < 70:
                    f.write("1. üî• **HIGH PRIORITY:** Complete Section 6.1 data preparation tasks\n")
                    f.write("2. üî• **HIGH PRIORITY:** Implement baseline models in Section 6.2\n")
                    f.write("3. üìà **MEDIUM:** Add comprehensive evaluation metrics\n")
                    f.write("4. üéØ **LOW:** Implement advanced XAI features\n")
                else:
                    f.write("1. üìà **MEDIUM:** Complete remaining evaluation metrics\n")
                    f.write("2. üéØ **LOW:** Implement SHAP/LIME for explainable AI\n")
                    f.write("3. üìù **LOW:** Add clinical patient examples analysis\n")
                    
            else:
                f.write("üéâ **All core tasks completed!** Consider implementing advanced features:\n")
                f.write("- Advanced time-series models with Darts\n")
                f.write("- Comprehensive SHAP analysis\n")
                f.write("- Clinical decision support interface\n")
                
            f.write("\n## üìö Academic Compliance\n\n")
            f.write("‚úÖ Professor-approved libraries only\n")
            f.write("‚úÖ Step-by-step implementation approach\n")
            f.write("‚úÖ Comprehensive documentation\n")
            f.write("‚úÖ Reproducible methodology\n")
            
        self.logger.info(f"üìÑ Comprehensive report saved: {report_path}")
        return report_path, overall_percentage
        
    def print_summary(self):
        """Print a concise, human-readable summary to console"""
        total_score = sum(section['score'] for section in self.results.values())
        total_max_score = sum(section['max_score'] for section in self.results.values())
        overall_percentage = (total_score / total_max_score * 100) if total_max_score > 0 else 0
        
        print("\n" + "="*80)
        print("üéì √úBUNG 5 - ML IMPLEMENTATION COMPLETION STATUS")
        print("="*80)
        print(f"üéØ Overall Progress: {total_score}/{total_max_score} points ({overall_percentage:.1f}%)")
        print()
        
        # Progress bar
        bar_length = 50
        filled_length = int(bar_length * overall_percentage / 100)
        bar = "‚ñà" * filled_length + "‚ñë" * (bar_length - filled_length)
        print(f"Progress: [{bar}] {overall_percentage:.1f}%")
        print()
        
        # Section breakdown with detailed status
        section_names = {
            '6.1': 'Data Preparation for ML',
            '6.2': 'Model Implementation & Training', 
            '6.3': 'Model Evaluation & Explainable AI'
        }
        
        for section_num, section_data in self.results.items():
            section_percentage = (section_data['score'] / section_data['max_score'] * 100) if section_data['max_score'] > 0 else 0
            
            if section_percentage >= 90:
                status_emoji = "üü¢"
                status_text = "EXCELLENT"
            elif section_percentage >= 80:
                status_emoji = "üü°"
                status_text = "GOOD"
            elif section_percentage >= 70:
                status_emoji = "ÔøΩ"
                status_text = "SATISFACTORY"
            elif section_percentage >= 50:
                status_emoji = "üî¥"
                status_text = "NEEDS WORK"
            else:
                status_emoji = "‚ùå"
                status_text = "INCOMPLETE"
            
            print(f"{status_emoji} Section {section_num}: {section_names[section_num]}")
            print(f"   Score: {section_data['score']}/{section_data['max_score']} ({section_percentage:.1f}%) - {status_text}")
            
            # Show completed tasks
            completed_tasks = [task for task, data in section_data['tasks'].items() if data['completed']]
            incomplete_tasks = [task for task, data in section_data['tasks'].items() if not data['completed']]
            
            if completed_tasks:
                print(f"   ‚úÖ Completed: {len(completed_tasks)} tasks")
                for task in completed_tasks[:2]:  # Show first 2
                    print(f"      ‚Ä¢ {task}")
                if len(completed_tasks) > 2:
                    print(f"      ‚Ä¢ ... and {len(completed_tasks) - 2} more")
            
            if incomplete_tasks:
                print(f"   ‚ùå Remaining: {len(incomplete_tasks)} tasks")
                for task in incomplete_tasks[:2]:  # Show first 2
                    print(f"      ‚Ä¢ {task}")
                if len(incomplete_tasks) > 2:
                    print(f"      ‚Ä¢ ... and {len(incomplete_tasks) - 2} more")
            print()
        
        print("="*80)
        
        # Overall status with actionable advice
        if overall_percentage >= 95:
            print("üèÜ STATUS: OUTSTANDING - Ready for presentation!")
            print("ÔøΩ Suggestion: Consider adding advanced features for excellence")
        elif overall_percentage >= 85:
            print("üéâ STATUS: EXCELLENT - Almost perfect implementation!")
            print("üí° Suggestion: Polish remaining details for perfection")
        elif overall_percentage >= 75:
            print("ÔøΩ STATUS: GOOD - Solid implementation with minor gaps")
            print("üí° Suggestion: Focus on completing XAI and patient analysis")
        elif overall_percentage >= 65:
            print("ÔøΩ STATUS: SATISFACTORY - Core requirements mostly met")
            print("üí° Suggestion: Enhance evaluation metrics and add XAI features")
        elif overall_percentage >= 50:
            print("‚ö†Ô∏è  STATUS: NEEDS IMPROVEMENT - Missing key components")
            print("üí° Suggestion: Focus on model training and basic evaluation first")
        else:
            print("üö® STATUS: INCOMPLETE - Significant work needed")
            print("üí° Suggestion: Start with data preparation and basic models")
        
        print()
        print("üìä Implementation Highlights:")
        
        # Count various achievements
        total_tasks = sum(len(section['tasks']) for section in self.results.values())
        completed_tasks = sum(len([t for t in section['tasks'].values() if t['completed']]) 
                             for section in self.results.values())
        
        print(f"   ‚úÖ Tasks Completed: {completed_tasks}/{total_tasks}")
        
        # Check for specific implementations
        xai_dir = os.path.join(self.viz_dir, 'xai')
        if os.path.exists(xai_dir) and len(os.listdir(xai_dir)) > 0:
            xai_viz_count = len([f for f in os.listdir(xai_dir) if f.endswith('.png')])
            print(f"   üîç XAI Visualizations: {xai_viz_count} files generated")
        
        if os.path.exists(os.path.join(project_root, 'models')) and len(os.listdir(os.path.join(project_root, 'models'))) > 0:
            model_count = len([f for f in os.listdir(os.path.join(project_root, 'models')) if f.endswith('.pkl')])
            print(f"   ü§ñ Models Trained: {model_count} models saved")
        
        total_viz = 0
        if os.path.exists(self.viz_dir):
            for root, dirs, files in os.walk(self.viz_dir):
                total_viz += len([f for f in files if f.endswith('.png')])
        print(f"   üìä Total Visualizations: {total_viz} files")
        
        total_reports = 0
        if os.path.exists(self.reports_dir):
            total_reports = len([f for f in os.listdir(self.reports_dir) 
                               if f.startswith('ml_') and f.endswith('.md')])
        print(f"   üìÑ Reports Generated: {total_reports} reports")
        
        print("="*80)
        print("üìã Next Steps:")
        
        if overall_percentage < 100:
            if overall_percentage < 70:
                print("üî• HIGH PRIORITY:")
                print("   1. Complete basic model training and evaluation")
                print("   2. Implement comprehensive metrics (AUC, F1, Precision, Recall)")
                print("   3. Add class imbalance handling (SMOTE)")
            elif overall_percentage < 85:
                print("üìà MEDIUM PRIORITY:")
                print("   1. Implement comprehensive XAI with SHAP")
                print("   2. Add patient-level explanations")
                print("   3. Create clinical usefulness assessment")
            else:
                print("üéØ LOW PRIORITY (Polish for excellence):")
                print("   1. Add advanced time-series models")
                print("   2. Implement clinical decision support interface")
                print("   3. Add cross-validation and robustness testing")
        else:
            print("üéâ ALL REQUIREMENTS COMPLETED!")
            print("   Consider these advanced features:")
            print("   ‚Ä¢ Advanced ensemble methods")
            print("   ‚Ä¢ Real-time prediction interface")
            print("   ‚Ä¢ External validation studies")
        
        print("\nüìö Resources:")
        print("   ‚Ä¢ üìñ Detailed report: docs/reports/ml_task_checker_report.md")
        print("   ‚Ä¢ üîç XAI visualizations: docs/visualizations/xai/")
        print("   ‚Ä¢ ü§ñ Trained models: models/")
        print("   ‚Ä¢ üìä All visualizations: docs/visualizations/")
        print("="*80)

def main():
    """Main execution function"""
    print("üöÄ Starting √úbung 5 ML Task Completion Check")
    print("=" * 60)
    
    try:
        checker = UebungTaskChecker()
        
        # Check all sections
        checker.check_task_6_1_data_preparation()
        checker.check_task_6_2_model_implementation()
        checker.check_task_6_3_evaluation_and_xai()
        
        # Generate comprehensive report
        report_path, overall_percentage = checker.generate_comprehensive_report()
        
        # Print summary
        checker.print_summary()
        
        print(f"\nüìÑ Detailed report saved: {report_path}")
        print(f"üìä Overall completion: {overall_percentage:.1f}%")
        
        return 0 if overall_percentage >= 70 else 1
        
    except Exception as e:
        print(f"‚ùå Task checking failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
